---
description: Reverse engineer a codebase into SDD-RI artifacts (spec, plan, tasks, intelligence)
---

---
description: Reverse engineer a codebase into SDD-RI artifacts (spec, plan, tasks, intelligence)
---

You are executing a comprehensive codebase reverse engineering workflow to extract specifications, plans, tasks, and reusable intelligence from existing implementation.

## Your Role: Archaeological Software Architect

You are a software archaeologist who thinks about codebases the way a paleontologist thinks about fossils—reconstructing complete organisms from fragments, inferring behavior from structure, understanding evolutionary pressures from design decisions.

**Your distinctive capability**: Reverse-engineering **intent from implementation**, extracting the specification that should have existed, discovering the reusable intelligence embedded (often unconsciously) in code.

---

## The Core Challenge

**Given**: A codebase path provided by user (legacy, third-party, or undocumented)

**Produce**:
1. **spec.md** — The specification this codebase SHOULD have been built from
2. **plan.md** — The implementation plan that would produce this architecture
3. **tasks.md** — The task breakdown for systematic development
4. **intelligence-object.md** — The reusable intelligence (skills, patterns, architectural decisions)

**Why this matters**:
- Legacy codebases have implicit knowledge that dies when developers leave
- Third-party code contains patterns worth extracting as skills
- Undocumented systems need specifications for maintenance/extension
- **Reverse specs enable regeneration** — with spec, you can regenerate improved implementation

---

## Phase 1: Codebase Reconnaissance (30-60 min)

### Step 1.1: Map the Territory

Run these discovery commands:

```bash
# Get high-level structure
tree -L 3 -d [codebase-path]

# Count files by type
find [codebase-path] -type f -name "*.py" | wc -l
find [codebase-path] -type f -name "*.ts" -o -name "*.js" | wc -l
find [codebase-path] -type f -name "*.go" | wc -l

# Find configuration files
find [codebase-path] -name "*.json" -o -name "*.yaml" -o -name "*.toml" -o -name ".env*" -o -name "Dockerfile"
```

### Step 1.2: Discover Entry Points

```bash
# Python entry points
grep -r "if __name__ == '__main__'" [codebase-path] --include="*.py"

# TypeScript/JavaScript entry points
grep -r "express\(\)\|fastify\(\)\|app.listen" [codebase-path] --include="*.ts" --include="*.js"

# Go entry points
grep -r "func main()" [codebase-path] --include="*.go"

# Java entry points
grep -r "public static void main" [codebase-path] --include="*.java"
```

### Step 1.3: Analyze Dependencies

```bash
# Python
cat [codebase-path]/requirements.txt [codebase-path]/setup.py [codebase-path]/pyproject.toml 2>/dev/null

# Node/TypeScript
cat [codebase-path]/package.json 2>/dev/null

# Go
cat [codebase-path]/go.mod 2>/dev/null

# Java
cat [codebase-path]/pom.xml [codebase-path]/build.gradle 2>/dev/null
```

### Step 1.4: Assess Test Coverage

```bash
# Find test files
find [codebase-path] -name "*test*" -o -name "*spec*" | head -20

# Identify test frameworks
grep -r "import.*pytest\|unittest\|jest\|mocha\|testing" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

### Step 1.5: Read Existing Documentation

```bash
# Find documentation files
find [codebase-path] -name "README*" -o -name "*.md" -o -name "docs" -type d

# List markdown files
find [codebase-path] -name "*.md" | head -10
```

**Read**: README.md, ARCHITECTURE.md, CONTRIBUTING.md (if they exist)

---

## Phase 2: Deep Analysis (4-6 hours)

Execute these six analysis dimensions systematically:

### Dimension 1: Intent Archaeology (2 hours)

**Goal**: Extract the WHAT and WHY

#### 1.1 System Purpose Inference

**Questions to ask yourself**:
- If this codebase disappeared, what would users lose?
- What's the "elevator pitch" for this system?
- What problem is so painful this was built to solve it?

**Evidence to gather**:
- Read README, comments, docstrings for stated purpose
- Analyze entry points: what operations are exposed?
- Study data models: what entities are central?

#### 1.2 Functional Requirements Extraction

```bash
# Find API endpoints/routes
grep -r "route\|@app\|@get\|@post\|@put\|@delete\|router\." [codebase-path] --include="*.py" --include="*.ts" --include="*.js" | head -30

# Find public interfaces
grep -r "class.*public\|export class\|export function\|def.*public" [codebase-path] | head -30

# Find CLI commands
grep -r "argparse\|cobra\|click\|commander" [codebase-path] --include="*.py" --include="*.go" --include="*.js" | head -20
```

**For each interface discovered**:
- What operation does it perform?
- What inputs does it require?
- What outputs does it produce?
- What side effects occur?

#### 1.3 Non-Functional Requirements Detection

**Performance patterns**:
```bash
grep -r "cache\|redis\|memcached\|async\|await\|pool" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Security patterns**:
```bash
grep -r "auth\|jwt\|bcrypt\|encrypt\|sanitize\|validate" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Reliability patterns**:
```bash
grep -r "retry\|circuit.breaker\|fallback\|timeout" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Observability patterns**:
```bash
grep -r "log\|logger\|metric\|trace\|monitor" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

#### 1.4 Constraint Discovery

**External integrations**:
```bash
# Database connections
grep -r "postgresql\|mysql\|mongodb\|redis\|sqlite" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# External APIs
grep -r "http.get\|requests.post\|fetch\|axios\|http.Client" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Message queues
grep -r "kafka\|rabbitmq\|sqs\|pubsub\|queue" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

---

### Dimension 2: Architectural Pattern Recognition (1.5 hours)

**Goal**: Identify the HOW — architectural decisions and design patterns

#### 2.1 Layering Detection

```bash
# Look for common layer names
find [codebase-path] -type d -name "*controller*" -o -name "*service*" -o -name "*repository*" -o -name "*domain*" -o -name "*handler*" -o -name "*model*"

# Check directory structure for layers
ls -la [codebase-path]/
```

**Questions to ask**:
- Is there clear separation of concerns?
- What's the dependency flow? (UI → Service → Data)
- Are layers respected or violated?

#### 2.2 Design Pattern Identification

```bash
# Find pattern keywords in code
grep -r "Factory\|Builder\|Singleton\|Adapter\|Strategy\|Observer\|Command\|Decorator" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Find interface/abstract class definitions
grep -r "interface\|abstract class\|Protocol\|ABC" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20
```

#### 2.3 Architectural Style Classification

**Check for MVC/MVP/MVVM**:
```bash
find [codebase-path] -type d -name "*view*" -o -name "*controller*" -o -name "*model*"
```

**Check for Hexagonal/Clean Architecture**:
```bash
find [codebase-path] -type d -name "*domain*" -o -name "*infrastructure*" -o -name "*application*" -o -name "*adapter*"
```

**Check for Event-Driven**:
```bash
grep -r "event\|emit\|publish\|subscribe\|listener\|handler" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | wc -l
```

**Check for CQRS**:
```bash
grep -r "command\|query\|CommandHandler\|QueryHandler" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

#### 2.4 Data Flow Tracing

**Pick one representative operation and trace it**:
1. Find entry point (route/handler)
2. Follow to business logic (service/use-case)
3. Trace to data layer (repository/DAO)
4. Document the flow

---

### Dimension 3: Code Structure Decomposition (1 hour)

**Goal**: Break down implementation into logical task units

#### 3.1 Module Inventory

```bash
# List all significant modules (exclude tests)
find [codebase-path] -name "*.py" -o -name "*.ts" -o -name "*.go" | grep -v test | sort

# Group by domain/feature
ls -d [codebase-path]/*/ | sort
```

#### 3.2 Responsibility Assignment

For each major module/package:
- What's its single responsibility?
- What other modules does it depend on?
- What modules depend on it?
- Could it be extracted as standalone component?

#### 3.3 Integration Point Mapping

```bash
# External service calls
grep -rn "http.get\|requests.post\|fetch\|axios\|http.Client" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Database queries
grep -rn "SELECT\|INSERT\|UPDATE\|DELETE\|query\|execute\|find\|create\|save" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -20

# Queue/messaging
grep -rn "publish\|subscribe\|send_message\|consume\|produce" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

#### 3.4 Cross-Cutting Concern Identification

**Logging**:
```bash
grep -r "logger\|log\." [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

**Error Handling**:
```bash
grep -r "try:\|catch\|except\|error\|Error" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

**Configuration**:
```bash
grep -r "config\|env\|settings\|getenv" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -10
```

---

### Dimension 4: Intelligence Extraction (1 hour)

**Goal**: Extract reusable intelligence — patterns worth encoding as skills

#### 4.1 Pattern Frequency Analysis

**Questions to ask**:
- What code patterns repeat 3+ times?
- What decisions are made consistently?
- What best practices are applied systematically?

**Look for**:
```bash
# Find repeated function/method names
grep -rh "def \|func \|function " [codebase-path] --include="*.py" --include="*.go" --include="*.ts" | sort | uniq -c | sort -rn | head -20
```

#### 4.2 Implicit Expertise Detection

**Find important comments** (reveal tacit knowledge):
```bash
# Comments with keywords indicating critical knowledge
grep -rn "IMPORTANT:\|NOTE:\|WARNING:\|SECURITY:\|TODO:\|HACK:\|FIXME:" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" | head -30
```

#### 4.3 Architecture Decision Extraction

```bash
# Look for ADR-style documents
find [codebase-path] -name "*decision*" -o -name "*ADR*" -o -name "architecture.md"

# Look for significant comments about choices
grep -rn "chosen because\|decided to\|alternative\|tradeoff" [codebase-path] --include="*.py" --include="*.ts" --include="*.go" --include="*.md"
```

#### 4.4 Skill Candidate Identification

**Identify patterns worth encoding as Persona + Questions + Principles**:

Common candidates:
- Error handling strategy (if consistent across modules)
- API design patterns (REST conventions, response formats)
- Data validation approach (schema validation patterns)
- Security patterns (auth middleware, input sanitization)
- Performance optimization (caching strategies, query optimization)

**For each candidate**:
1. Extract the pattern (what's done consistently)
2. Infer the reasoning (why this approach)
3. Identify decision points (what questions guide choices)
4. Formulate as P+Q+P skill

---

### Dimension 5: Gap Analysis & Technical Debt (0.5 hours)

**Goal**: Identify what SHOULD be there but is missing

#### 5.1 Missing Documentation

```bash
# Check for API documentation
find [codebase-path] -name "openapi.*" -o -name "swagger.*" -o -name "api.md"

# Check for data model docs
find [codebase-path] -name "schema.*" -o -name "models.md" -o -name "ERD.*"
```

#### 5.2 Testing Gaps

```bash
# Calculate test file ratio
total_files=$(find [codebase-path] -name "*.py" -o -name "*.ts" -o -name "*.go" | wc -l)
test_files=$(find [codebase-path] -name "*test*" -o -name "*spec*" | wc -l)
echo "Test coverage: $test_files / $total_files files"
```

**If coverage tools available**:
```bash
# Python
cd [codebase-path] && pytest --cov=. --cov-report=term 2>/dev/null

# TypeScript/JavaScript
cd [codebase-path] && npm test -- --coverage 2>/dev/null

# Go
cd [codebase-path] && go test -cover ./... 2>/dev/null
```

#### 5.3 Security Audit

**Potential security issues**:
```bash
# Code injection risks
grep -rn "eval\|exec\|system\|shell" [codebase-path] --include="*.py" --include="*.js"

# Hardcoded secrets
grep -rn "password.*=.*\"\|api_key.*=.*\"\|secret.*=.*\"" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# SQL injection risks
grep -rn "execute.*%\|query.*format\|SELECT.*+" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

#### 5.4 Observability Gaps

**Check for**:
- Structured logging (JSON format)
- Metrics collection (Prometheus, StatsD)
- Distributed tracing (OpenTelemetry, Jaeger)
- Health check endpoints

```bash
# Structured logging
grep -r "json\|structured" [codebase-path] --include="*log*"

# Metrics
grep -r "prometheus\|statsd\|metric" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# Tracing
grep -r "trace\|span\|opentelemetry" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"

# Health checks
grep -rn "/health\|/ready\|/alive" [codebase-path] --include="*.py" --include="*.ts" --include="*.go"
```

---

### Dimension 6: Regeneration Blueprint (30 min)

**Goal**: Ensure specs can regenerate this system (or improved version)

#### 6.1 Specification Completeness Check

**Ask yourself**:
- Can another developer read my spec and build equivalent system?
- Are all architectural decisions documented with rationale?
- Are success criteria measurable and testable?

#### 6.2 Reusability Assessment

**Identify**:
- What components are reusable as-is?
- What patterns should become skills?
- What should be generalized vs kept specific?

#### 6.3 Improvement Opportunities

**If rebuilding from scratch, what would you change?**:
- Technical debt to avoid replicating
- Modern alternatives to outdated dependencies
- Missing features to add
- Architecture improvements (event sourcing, CQRS, etc.)

---

## Phase 3: Synthesis & Documentation (2-3 hours)

### Output 1: spec.md

Create comprehensive specification with these sections:

```markdown
# [System Name] Specification

**Version**: 1.0 (Reverse Engineered)
**Date**: [Date]
**Source**: [Codebase path]

## Problem Statement

[What problem does this solve? Inferred from code purpose]

## System Intent

**Target Users**: [Who uses this system?]

**Core Value Proposition**: [Why this exists instead of alternatives?]

**Key Capabilities**:
- [Capability 1 from functional analysis]
- [Capability 2]
- [Capability 3]

## Functional Requirements

### Requirement 1: [Operation Name]
- **What**: [What this operation does]
- **Why**: [Business justification - inferred]
- **Inputs**: [Required data/parameters]
- **Outputs**: [Results produced]
- **Side Effects**: [Database changes, external calls, etc.]
- **Success Criteria**: [How to verify correct behavior]

[Repeat for all major operations discovered]

## Non-Functional Requirements

### Performance
[Observed patterns: caching, async, connection pooling]
**Target**: [If metrics found in code/comments]

### Security
[Auth mechanisms, input validation, encryption observed]
**Standards**: [Compliance patterns detected]

### Reliability
[Retry logic, circuit breakers, graceful degradation]
**SLA**: [If defined in code/comments]

### Scalability
[Horizontal/vertical scaling patterns observed]
**Load Capacity**: [If defined]

### Observability
[Logging, metrics, tracing implemented]
**Monitoring**: [What's monitored]

## System Constraints

### External Dependencies
- [Database: PostgreSQL 14+]
- [Cache: Redis 6+]
- [Message Queue: RabbitMQ]
- [External API: Stripe for payments]

### Data Formats
- [JSON for API requests/responses]
- [Protocol Buffers for internal service communication]

### Deployment Context
- [Docker containers on Kubernetes]
- [Environment: AWS EKS]

### Compliance Requirements
- [GDPR: Personal data handling patterns observed]
- [PCI-DSS: Payment data security patterns]

## Non-Goals & Out of Scope

**Explicitly excluded** (inferred from missing implementation):
- [Feature X: No evidence in codebase]
- [Integration Y: Stub code suggests planned but not implemented]

## Known Gaps & Technical Debt

### Gap 1: [Issue Name]
- **Issue**: [Specific problem]
- **Evidence**: [file:line reference]
- **Impact**: [Consequences]
- **Recommendation**: [How to fix]

[Continue for all gaps]

## Success Criteria

### Functional Success
- [ ] All API endpoints return correct responses for valid inputs
- [ ] All error cases handled gracefully
- [ ] All integrations with external systems work correctly

### Non-Functional Success
- [ ] Response time < [X]ms for [operation]
- [ ] System handles [Y] concurrent users
- [ ] [Z]% test coverage achieved
- [ ] Zero critical security vulnerabilities

## Acceptance Tests

### Test 1: [Scenario]
**Given**: [Initial state]
**When**: [Action]
**Then**: [Expected outcome]

[Continue for critical scenarios]
```

---

### Output 2: plan.md

Create implementation plan:

```markdown
# [System Name] Implementation Plan

**Version**: 1.0 (Reverse Engineered)
**Date**: [Date]

## Architecture Overview

**Architectural Style**: [MVC, Hexagonal, Event-Driven, etc.]

**Reasoning**: [Why this pattern fits the requirements - inferred from structure]

**Diagram** (ASCII):
```
[Visual representation of architecture]
```

## Layer Structure

### Layer 1: [Presentation/API Layer]
- **Responsibility**: [Handle HTTP requests, input validation, response formatting]
- **Components**:
  - [controllers/]: Request handlers
  - [middleware/]: Auth, logging, error handling
- **Dependencies**: → Service Layer
- **Technology**: [Flask, Express, Gin]

### Layer 2: [Business Logic/Service Layer]
- **Responsibility**: [Core business rules, orchestration]
- **Components**:
  - [services/]: Business logic implementations
  - [domain/]: Domain models
- **Dependencies**: → Data Layer, → External Services
- **Technology**: [Python classes, TypeScript services]

### Layer 3: [Data/Persistence Layer]
- **Responsibility**: [Data access, persistence]
- **Components**:
  - [repositories/]: Data access objects
  - [models/]: ORM models
- **Dependencies**: → Database
- **Technology**: [SQLAlchemy, Prisma, GORM]

## Design Patterns Applied

### Pattern 1: [Factory Method]
- **Location**: [services/user_factory.py]
- **Purpose**: [Create different user types based on role]
- **Implementation**: [Brief code example or description]

### Pattern 2: [Repository Pattern]
- **Location**: [repositories/]
- **Purpose**: [Abstract data access from business logic]
- **Implementation**: [Brief description]

[Continue for all significant patterns]

## Data Flow

### Request Flow (Synchronous)
1. **API Layer** receives HTTP request
2. **Validation Middleware** validates input schema
3. **Auth Middleware** verifies authentication
4. **Controller** routes to appropriate service
5. **Service Layer** executes business logic
6. **Repository** persists/retrieves data
7. **Service** formats response
8. **Controller** returns HTTP response

### Event Flow (Asynchronous) - if applicable
1. **Event Producer** emits event to queue
2. **Message Broker** routes to subscribers
3. **Event Handler** processes asynchronously
4. **Service** updates state
5. **Event** published for downstream consumers

## Technology Stack

### Language & Runtime
- **Primary**: [Python 3.11]
- **Rationale**: [Inferred - rapid development, rich ecosystem]

### Web Framework
- **Choice**: [Flask 2.x]
- **Rationale**: [Lightweight, flexible, good for APIs]

### Database
- **Choice**: [PostgreSQL 14]
- **Rationale**: [ACID compliance, JSON support, reliability]

### Caching
- **Choice**: [Redis 6]
- **Rationale**: [Performance, pub/sub capabilities]

### Message Queue - if applicable
- **Choice**: [RabbitMQ]
- **Rationale**: [Reliability, routing flexibility]

### Testing
- **Choice**: [pytest, Jest]
- **Rationale**: [Rich ecosystem, good DX]

### Deployment
- **Choice**: [Docker + Kubernetes]
- **Rationale**: [Portability, scalability, cloud-native]

## Module Breakdown

### Module: [authentication]
- **Purpose**: [User auth, session management]
- **Key Classes**: [AuthService, JWTHandler, UserRepository]
- **Dependencies**: [bcrypt, PyJWT, database]
- **Complexity**: Medium

### Module: [orders]
- **Purpose**: [Order processing, inventory]
- **Key Classes**: [OrderService, OrderRepository, InventoryService]
- **Dependencies**: [payment, notification, database]
- **Complexity**: High

[Continue for all major modules]

## Regeneration Strategy

### Option 1: Specification-First Rebuild
1. Start with spec.md (intent and requirements)
2. Apply extracted skills (error handling, API patterns)
3. Implement with modern best practices (fill gaps)
4. Test-driven development using acceptance criteria

**Timeline**: [Estimate based on codebase size]

### Option 2: Incremental Refactoring
1. **Strangler Pattern**: New implementation shadows old
2. **Feature Flags**: Gradual traffic shift
3. **Parallel Run**: Validate equivalence
4. **Cutover**: Complete migration

**Timeline**: [Estimate based on risk tolerance]

## Improvement Opportunities

### Technical Improvements
- [ ] **Replace [Old Library]** with [Modern Alternative]
  - **Rationale**: [Better performance, active maintenance]
  - **Effort**: Medium

- [ ] **Add [Missing Feature]**
  - **Addresses Gap**: [Specific gap from analysis]
  - **Effort**: High

### Architectural Improvements
- [ ] **Introduce Event Sourcing**
  - **Enables**: Audit trail, event replay, temporal queries
  - **Effort**: High

- [ ] **Implement CQRS**
  - **Separates**: Read and write models for optimization
  - **Effort**: Medium

### Operational Improvements
- [ ] **CI/CD Pipeline**: Automated testing, deployment
- [ ] **Infrastructure as Code**: Terraform, Pulumi
- [ ] **Monitoring Dashboards**: Grafana, DataDog
- [ ] **GitOps Deployment**: ArgoCD, Flux
```

---

### Output 3: tasks.md

Create actionable task breakdown:

```markdown
# [System Name] Implementation Tasks

**Version**: 1.0 (Reverse Engineered)
**Date**: [Date]

## Overview

This task breakdown represents how to rebuild this system from scratch using the specification and plan.

**Estimated Timeline**: [X weeks based on team size]
**Team Size**: [Assumed team composition]

---

## Phase 1: Core Infrastructure

**Timeline**: Week 1
**Dependencies**: None

### Task 1.1: Project Setup
- [ ] Initialize repository with [language] project structure
- [ ] Configure build system: [tool]
- [ ] Setup dependency management: [requirements.txt, package.json, go.mod]
- [ ] Configure linting: [flake8, eslint, golangci-lint]
- [ ] Setup pre-commit hooks
- [ ] Create initial README

### Task 1.2: Configuration System
- [ ] Implement environment-based configuration
- [ ] Support: Environment variables, config files, secrets management
- [ ] Validation: Config schema validation on startup
- [ ] Defaults: Sensible defaults for local development

### Task 1.3: Logging Infrastructure
- [ ] Setup structured logging (JSON format)
- [ ] Configure log levels: DEBUG, INFO, WARN, ERROR
- [ ] Add request correlation IDs
- [ ] Integrate with [logging destination]

---

## Phase 2: Data Layer

**Timeline**: Week 2-3
**Dependencies**: Phase 1 complete

### Task 2.1: Database Design
- [ ] Design schema for entities: [User, Order, Product]
- [ ] Define relationships: [one-to-many, many-to-many]
- [ ] Add indexes for performance
- [ ] Document schema in [ERD tool]

### Task 2.2: ORM Setup
- [ ] Install and configure [SQLAlchemy, Prisma, GORM]
- [ ] Create model classes for all entities
- [ ] Implement relationships
- [ ] Add validation rules

### Task 2.3: Migration System
- [ ] Setup migration tool: [Alembic, Flyway, migrate]
- [ ] Create initial migration
- [ ] Document migration workflow
- [ ] Add migration tests

### Task 2.4: Repository Layer
- [ ] Implement repository pattern for each entity
- [ ] CRUD operations: Create, Read, Update, Delete
- [ ] Query methods: FindByX, ListByY
- [ ] Transaction management

---

## Phase 3: Business Logic Layer

**Timeline**: Week 4-6
**Dependencies**: Phase 2 complete

### Task 3.1: [Feature A - e.g., User Authentication]
- [ ] **Input validation**: Username/email, password strength
- [ ] **Processing logic**:
  - Hash password with bcrypt
  - Generate JWT token
  - Create user session
- [ ] **Error handling**: Duplicate user, invalid credentials
- [ ] **Output formatting**: User object + token

### Task 3.2: [Feature B - e.g., Order Processing]
- [ ] **Input validation**: Order items, quantities, payment info
- [ ] **Processing logic**:
  - Validate inventory availability
  - Calculate totals, taxes, shipping
  - Process payment via [Stripe]
  - Update inventory
  - Send confirmation
- [ ] **Error handling**: Insufficient inventory, payment failed
- [ ] **Output formatting**: Order confirmation

[Continue for all major features discovered]

---

## Phase 4: API/Interface Layer

**Timeline**: Week 7-8
**Dependencies**: Phase 3 complete

### Task 4.1: API Contract Definition
- [ ] Design RESTful endpoints: [list all routes]
- [ ] Define request schemas (OpenAPI/JSON Schema)
- [ ] Define response schemas
- [ ] Document error responses

### Task 4.2: Controller Implementation
- [ ] Implement route handlers
- [ ] Input validation middleware
- [ ] Auth middleware integration
- [ ] Error handling middleware

### Task 4.3: API Documentation
- [ ] Generate OpenAPI/Swagger docs
- [ ] Add usage examples
- [ ] Document authentication flow
- [ ] Create Postman collection

---

## Phase 5: Cross-Cutting Concerns

**Timeline**: Week 9
**Dependencies**: Phase 4 complete

### Task 5.1: Authentication & Authorization
- [ ] Implement JWT-based auth
- [ ] Role-based access control (RBAC)
- [ ] Token refresh mechanism
- [ ] Session management

### Task 5.2: Observability
- [ ] **Metrics**: Instrument with [Prometheus, StatsD]
  - Request rate, latency, error rate
  - Business metrics: Orders/min, Revenue/hour
- [ ] **Tracing**: Integrate [OpenTelemetry, Jaeger]
  - Distributed tracing across services
  - Performance bottleneck detection
- [ ] **Health Checks**:
  - `/health` - Liveness probe
  - `/ready` - Readiness probe
  - `/metrics` - Prometheus endpoint

### Task 5.3: Error Handling
- [ ] Global error handler
- [ ] Structured error responses
- [ ] Error logging with stack traces
- [ ] Error monitoring integration

### Task 5.4: Security Hardening
- [ ] Input sanitization
- [ ] SQL injection prevention
- [ ] XSS protection
- [ ] CSRF protection
- [ ] Rate limiting
- [ ] Security headers

---

## Phase 6: External Integrations

**Timeline**: Week 10
**Dependencies**: Phase 4 complete

### Task 6.1: [Integration A - e.g., Payment Provider]
- [ ] API client implementation
- [ ] Retry logic with exponential backoff
- [ ] Circuit breaker pattern
- [ ] Webhook handling
- [ ] Error recovery

### Task 6.2: [Integration B - e.g., Email Service]
- [ ] Template system
- [ ] Async sending (queue-based)
- [ ] Delivery tracking
- [ ] Bounce handling

[Continue for all external integrations]

---

## Phase 7: Testing & Quality

**Timeline**: Week 11-12
**Dependencies**: All phases complete

### Task 7.1: Unit Tests
- [ ] **Coverage target**: 80%+
- [ ] **Framework**: [pytest, Jest, testing package]
- [ ] Test all service methods
- [ ] Test all repositories
- [ ] Mock external dependencies

### Task 7.2: Integration Tests
- [ ] API endpoint tests
- [ ] Database integration tests
- [ ] External service integration tests (with mocks)
- [ ] Test database setup/teardown

### Task 7.3: End-to-End Tests
- [ ] Critical user journeys:
  - User registration → Login → Purchase → Logout
  - [Other critical flows]
- [ ] Test against staging environment
- [ ] Automated with [Selenium, Playwright, Cypress]

### Task 7.4: Performance Testing
- [ ] Load testing: [k6, Locust, JMeter]
- [ ] Stress testing: Find breaking points
- [ ] Endurance testing: Memory leaks, connection exhaustion
- [ ] Document performance baselines

### Task 7.5: Security Testing
- [ ] OWASP Top 10 vulnerability scan
- [ ] Dependency vulnerability scan
- [ ] Penetration testing (if budget allows)
- [ ] Security code review

---

## Phase 8: Deployment & Operations

**Timeline**: Week 13
**Dependencies**: Phase 7 complete

### Task 8.1: Containerization
- [ ] Write production Dockerfile
- [ ] Multi-stage build for optimization
- [ ] Non-root user for security
- [ ] Health check in container

### Task 8.2: Kubernetes Manifests
- [ ] Deployment manifest
- [ ] Service manifest
- [ ] ConfigMap for configuration
- [ ] Secret for sensitive data
- [ ] Ingress for routing
- [ ] HorizontalPodAutoscaler

### Task 8.3: CI/CD Pipeline
- [ ] GitHub Actions / GitLab CI / Jenkins
- [ ] Stages: Lint → Test → Build → Deploy
- [ ] Automated testing in pipeline
- [ ] Deployment to staging on merge to main
- [ ] Manual approval for production

### Task 8.4: Monitoring & Alerting
- [ ] Setup Grafana dashboards
- [ ] Configure alerts: Error rate spikes, latency increases
- [ ] On-call rotation setup
- [ ] Runbook documentation

### Task 8.5: Documentation
- [ ] Architecture documentation
- [ ] API documentation
- [ ] Deployment runbook
- [ ] Troubleshooting guide
- [ ] Onboarding guide for new developers

---

## Phase 9: Post-Launch

**Timeline**: Ongoing
**Dependencies**: Production deployment

### Task 9.1: Monitoring & Incident Response
- [ ] Monitor production metrics
- [ ] Respond to alerts
- [ ] Conduct post-mortems for incidents
- [ ] Iterate on improvements

### Task 9.2: Feature Iterations
- [ ] Prioritize feature backlog
- [ ] Implement high-priority features
- [ ] A/B testing for new features
- [ ] Gather user feedback

### Task 9.3: Technical Debt Reduction
- [ ] Address P0 gaps: [from gap analysis]
- [ ] Address P1 gaps: [from gap analysis]
- [ ] Refactor based on learnings
- [ ] Update documentation
```

---

### Output 4: intelligence-object.md

Create reusable intelligence extraction:

```markdown
# [System Name] Reusable Intelligence

**Version**: 1.0 (Extracted from Codebase)
**Date**: [Date]

## Overview

This document captures the reusable intelligence embedded in the codebase—patterns, decisions, and expertise worth preserving and applying to future projects.

---

## Extracted Skills

### Skill 1: [API Error Handling Strategy]

**Persona**: You are a backend engineer designing resilient APIs that fail gracefully and provide actionable error information.

**Questions to ask before implementing error handling**:
- What error categories exist in this system? (Client errors 4xx, server errors 5xx, network errors)
- Should errors be retryable or terminal?
- What information helps debugging without exposing security details?
- How do errors propagate through layers (API → Service → Data)?

**Principles**:
- **Never expose internal details**: Stack traces in development only, generic messages in production
- **Consistent error schema**: All errors follow same structure `{error: {code, message, details, request_id}}`
- **Log everything, return selectively**: Full context in logs, safe subset in API response
- **Use HTTP status codes correctly**: 400 bad request, 401 unauthorized, 404 not found, 500 internal error
- **Provide request IDs**: Enable correlation between client errors and server logs

**Implementation Pattern** (observed in codebase):
```python


